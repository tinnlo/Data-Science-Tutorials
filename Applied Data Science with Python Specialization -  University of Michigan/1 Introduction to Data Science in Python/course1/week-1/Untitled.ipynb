{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740e33da-24f6-4252-aa74-b1107e268d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a4 = np.arange(1, 4, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf9a2a9-2123-4f9e-b571-d1f8d0b342b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bd10e8-0da0-4f63-bbb6-cd74887222dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4.ndim == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e30a704-4f6b-4241-95d2-923e96862e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "text=r'''Everyone has the following fundamental freedoms:\n",
    "    (a) freedom of conscience and religion;\n",
    "    (b) freedom of thought, belief, opinion and expression, including freedom of the press and other media of communication;\n",
    "    (c) freedom of peaceful assembly; and\n",
    "    (d) freedom of association.'''\n",
    "\n",
    "import re\n",
    "pattern = '\\(.\\)'\n",
    "print(len(re.findall(pattern,text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2776a8-051a-4604-9f56-dfca101d8dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "old = np.array([[1, 1, 1], [1, 1, 1]])\n",
    "new = old\n",
    "new[0, :2] = 0\n",
    "\n",
    "print(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb793736-1e1e-4371-8ef1-63cc9789beb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60787067, 0.2298354 , 0.69321736, 0.99382514])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.random.rand(4)\n",
    "a1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c14034a-d24b-471b-b1f2-467d142737a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a5 = np.linspace(1 ,4, 4)\n",
    "a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8f397e7-bd5b-484c-bfef-91613ceadef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amy', 'Mary', 'Ruth', 'Peter']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def names():\n",
    "    simple_string = \"\"\"Amy is 5 years old, and her sister Mary is 2 years old. \n",
    "    Ruth and Peter, their parents, have 3 kids.\"\"\"\n",
    "\n",
    "    # Define a regular expression to match names\n",
    "    name_pattern = r'\\b[A-Z][a-z]*\\b'\n",
    "\n",
    "    # Use findall to extract all matches\n",
    "    name_list = re.findall(name_pattern, simple_string)\n",
    "\n",
    "    return name_list\n",
    "\n",
    "result = names()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f53bc2f0-b9ae-4b5f-bc1b-63d2affdbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(names()) == 4, \"There are four names in the simple_string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d9a14fd-4c08-4165-9d35-ea02f871b125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(8)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646711ba-47c1-4113-922f-bdb5e234a5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3, 40, 40,  6,  7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[4:6]\n",
    "b[:] = 40\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee649d2a-bba2-4a8c-9187-346eed83e10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[4] + a[6]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "636f240a-f76f-44a7-9f1c-2c680d598f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s = 'ABCAC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2029de2c-0455-4dd6-b6cb-4af860ea3a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(re.match('A', s)) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657817f3-e14f-497e-a910-a0cf0b46eb76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assets/nhl.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m nhl_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massets/nhl.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m cities \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/wikipedia_data.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Exclude the last row of \"Totals\", and select the columns we need.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/nhl.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "nhl_df = pd.read_csv(\"assets/nhl.csv\")\n",
    "cities = pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "# Exclude the last row of \"Totals\", and select the columns we need.\n",
    "cities = cities.iloc[:-1, [0, 3, 5, 6, 7, 8]] \n",
    "\n",
    "def clean_nhl_df(df=nhl_df):\n",
    "    # Remove \"*\" from the team names\n",
    "    nhl_df['team'] = nhl_df['team'].str.rstrip('*')\n",
    "    \n",
    "    # Remove rows corresponding to divisions\n",
    "    nhl_df_cleaned = nhl_df[~nhl_df['team'].str.contains('Division')]\n",
    "    \n",
    "    # Filter NHL data for the year 2018\n",
    "    return nhl_df_cleaned[nhl_df_cleaned[\"year\"] == 2018]\n",
    "\n",
    "def clean_cities_column(team):\n",
    "    # Remove notes and extra spaces\n",
    "    return re.sub(r'\\[.*\\]|—\\s*', '', team)\n",
    "\n",
    "# Create a mapping between NHL team names and metropolitan areas\n",
    "team_mapping_nhl = {\n",
    "        'Tampa Bay Lightning': 'Tampa Bay Area',\n",
    "        'Boston Bruins': 'Boston',\n",
    "        'Toronto Maple Leafs': 'Toronto',\n",
    "        'Florida Panthers': 'Miami–Fort Lauderdale',\n",
    "        'Detroit Red Wings': 'Detroit',\n",
    "        'Montreal Canadiens': 'Montreal',\n",
    "        'Ottawa Senators': 'Ottawa',\n",
    "        'Buffalo Sabres': 'Buffalo',\n",
    "        'Washington Capitals': 'Washington, D.C.',\n",
    "        'Pittsburgh Penguins': 'Pittsburgh',\n",
    "        'Philadelphia Flyers': 'Philadelphia',\n",
    "        'Columbus Blue Jackets': 'Columbus',\n",
    "        'New Jersey Devils': 'New York City',\n",
    "        'Carolina Hurricanes': 'Raleigh',\n",
    "        'New York Islanders': 'New York City',\n",
    "        'New York Rangers': 'New York City',\n",
    "        'Nashville Predators': 'Nashville',\n",
    "        'Winnipeg Jets': 'Winnipeg',\n",
    "        'Minnesota Wild': 'Minneapolis–Saint Paul',\n",
    "        'Colorado Avalanche': 'Denver',\n",
    "        'St. Louis Blues': 'St. Louis',\n",
    "        'Dallas Stars': 'Dallas–Fort Worth',\n",
    "        'Chicago Blackhawks': 'Chicago',\n",
    "        'Vegas Golden Knights': 'Las Vegas',\n",
    "        'Anaheim Ducks': 'Los Angeles',\n",
    "        'San Jose Sharks': 'San Francisco Bay Area',\n",
    "        'Los Angeles Kings': 'Los Angeles',\n",
    "        'Calgary Flames': 'Calgary',\n",
    "        'Edmonton Oilers': 'Edmonton',\n",
    "        'Vancouver Canucks': 'Vancouver',\n",
    "        'Arizona Coyotes': 'Phoenix',\n",
    "}\n",
    "\n",
    "def nhl_correlation():\n",
    "    nhl_2018 = clean_nhl_df()\n",
    "    \n",
    "    # Clean the Big 4 columns in cities  \n",
    "    cities[\"NHL\"] = cities[\"NHL\"].apply(clean_cities_column).replace('', np.NaN)\n",
    "\n",
    "    # Create a new column for metropolitan area in the NHL dataset\n",
    "    nhl_2018[\"Metropolitan area\"] = nhl_2018[\"team\"].map(team_mapping_nhl)\n",
    "\n",
    "    # Merge NHL data with cities data\n",
    "    merged_df = pd.merge(nhl_2018, cities, how=\"left\", on=\"Metropolitan area\")\n",
    "    \n",
    "    # Rename the Population column\n",
    "    merged_df = merged_df.rename(columns={\"Population (2016 est.)[8]\": \"Population\"})\n",
    "    \n",
    "    # Convert columns to appropriate data types\n",
    "    merged_df[['W', 'L', 'Population']] = merged_df[['W', 'L', 'Population']].astype(float)\n",
    "    merged_df[\"WinLossRatio\"] = merged_df[\"W\"] / (merged_df[\"W\"] + merged_df[\"L\"])\n",
    "    \n",
    "    # Group by metropolitan area and calculate average win/loss ratio\n",
    "    win_loss_df = merged_df.groupby(\"Metropolitan area\").agg({'Population': 'mean', 'WinLossRatio': 'mean'})\n",
    "    \n",
    "    population_by_region = win_loss_df[\"Population\"]\n",
    "    win_loss_by_region   = win_loss_df[\"WinLossRatio\"]\n",
    "\n",
    "    assert len(population_by_region) == len(win_loss_by_region), \"Q1: Your lists must be the same length\"\n",
    "    assert len(population_by_region) == 28, \"Q1: There should be 28 teams being analysed for NHL\"\n",
    "\n",
    "    corr = stats.pearsonr(population_by_region, win_loss_by_region)[0]\n",
    "    return corr\n",
    "\n",
    "# Call the function\n",
    "correlation_result = nhl_correlation()\n",
    "print(correlation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e02e7-1c35-4f59-a32d-853f1b1b8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def clean_nba_df(df=nba_df):\n",
    "    # Remove \"*\" and other characters from the team names\n",
    "    nba_df['team'] = nba_df['team'].str.replace(r'\\*?\\s*\\((\\d+)\\)?$', '').str.strip()\n",
    "\n",
    "    # Filter NBA data for the year 2018\n",
    "    return nba_df[nba_df[\"year\"] == 2018]\n",
    "\n",
    "# Create a mapping between NBA team names and metropolitan areas\n",
    "team_mapping_nba = {\n",
    "        'Toronto Raptors': 'Toronto',\n",
    "        'Boston Celtics': 'Boston',\n",
    "        'Philadelphia 76ers': 'Philadelphia',\n",
    "        'Cleveland Cavaliers': 'Cleveland',\n",
    "        'Indiana Pacers': 'Indianapolis',\n",
    "        'Miami Heat': 'Miami–Fort Lauderdale',\n",
    "        'Milwaukee Bucks': 'Milwaukee',\n",
    "        'Washington Wizards': 'Washington, D.C.',\n",
    "        'Detroit Pistons': 'Detroit',\n",
    "        'Charlotte Hornets': 'Charlotte',\n",
    "        'New York Knicks': 'New York City',\n",
    "        'Brooklyn Nets': 'New York City',\n",
    "        'Chicago Bulls': 'Chicago',\n",
    "        'Orlando Magic': 'Orlando',\n",
    "        'Atlanta Hawks': 'Atlanta',\n",
    "        'Houston Rockets': 'Houston',\n",
    "        'Golden State Warriors': 'San Francisco Bay Area',\n",
    "        'Portland Trail Blazers': 'Portland',\n",
    "        'Oklahoma City Thunder': 'Oklahoma City',\n",
    "        'Utah Jazz': 'Salt Lake City',\n",
    "        'New Orleans Pelicans': 'New Orleans',\n",
    "        'San Antonio Spurs': 'San Antonio',\n",
    "        'Minnesota Timberwolves': 'Minneapolis–Saint Paul',\n",
    "        'Denver Nuggets': 'Denver',\n",
    "        'Los Angeles Clippers': 'Los Angeles',\n",
    "        'Los Angeles Lakers': 'Los Angeles',\n",
    "        'Sacramento Kings': 'Sacramento',\n",
    "        'Dallas Mavericks': 'Dallas–Fort Worth',\n",
    "        'Memphis Grizzlies': 'Memphis',\n",
    "        'Phoenix Suns': 'Phoenix'\n",
    "}\n",
    "\n",
    "def nba_correlation():\n",
    "    nba_2018 = clean_nba_df()\n",
    "\n",
    "    # Clean the Big 4 columns in cities\n",
    "    cities[\"NBA\"] = cities[\"NBA\"].apply(clean_cities_column).replace('', np.NaN)\n",
    "    \n",
    "    # Create a new column for metropolitan area in the NBA dataset\n",
    "    nba_2018[\"Metropolitan area\"] = nba_2018[\"team\"].map(team_mapping_nba)\n",
    "\n",
    "    # Merge NBA data with cities data\n",
    "    merged_df = pd.merge(nba_2018, cities, how=\"left\", on=\"Metropolitan area\")\n",
    "\n",
    "    # Rename the Population column\n",
    "    merged_df = merged_df.rename(columns={\"Population (2016 est.)[8]\": \"Population\"})\n",
    "    \n",
    "    # Convert columns to appropriate data types\n",
    "    merged_df[['W', 'L', 'Population']] = merged_df[['W', 'L', 'Population']].astype(float)\n",
    "    merged_df[\"WinLossRatio\"] = merged_df[\"W\"] / (merged_df[\"W\"] + merged_df[\"L\"])\n",
    "    \n",
    "    # Group by metropolitan area and calculate average win/loss ratio\n",
    "    win_loss_df = merged_df.groupby(\"Metropolitan area\").agg({'WinLossRatio': 'mean', 'Population': 'mean'})\n",
    "    \n",
    "    population_by_region = win_loss_df[\"Population\"] # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = win_loss_df[\"WinLossRatio\"] # pass in win/loss ratio from nba_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    assert len(population_by_region) == len(win_loss_by_region), \"Q2: Your lists must be the same length\"\n",
    "    assert len(population_by_region) == 28, \"Q2: There should be 28 teams being analysed for NBA\"\n",
    "\n",
    "    corr = stats.pearsonr(population_by_region, win_loss_by_region)[0]\n",
    "    return corr\n",
    "\n",
    "# Call the function\n",
    "correlation_result_nba = nba_correlation()\n",
    "print(correlation_result_nba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b6420-55df-441d-9491-219bf4ee01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "mlb_df=pd.read_csv(\"assets/mlb.csv\")\n",
    "nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "nfl_df=pd.read_csv(\"assets/nfl.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def clean_and_merge_df(league_name, league_df, cities_df):\n",
    "    # Clean the Big 4 columns in cities\n",
    "    cities_df[league_name] = cities_df[league_name].apply(clean_cities_column).replace('', np.NaN)\n",
    "\n",
    "    # Create a new column for metropolitan area in the dataset\n",
    "    if league_name == 'NHL':\n",
    "        league_df = clean_nhl_df(league_df)\n",
    "        league_df[\"Metropolitan area\"] = league_df[\"team\"].map(team_mapping_nhl)\n",
    "    elif league_name == 'NBA':\n",
    "        league_df = clean_nba_df(league_df)\n",
    "        league_df[\"Metropolitan area\"] = league_df[\"team\"].map(team_mapping_nba)\n",
    "    elif league_name == 'MLB':\n",
    "        league_df[\"Metropolitan area\"] = league_df[\"team\"].map(team_mapping_mlb)\n",
    "    elif league_name == 'NFL':\n",
    "        league_df = clean_nfl_df(league_df)\n",
    "        league_df[\"Metropolitan area\"] = league_df[\"team\"].map(team_mapping_nfl)\n",
    "    else:\n",
    "        print(\"Invalid League Name!\")\n",
    "\n",
    "    # Merge data with cities data\n",
    "    merged_df = pd.merge(league_df, cities_df, how=\"left\", on=\"Metropolitan area\")\n",
    "\n",
    "    # Rename the Population column\n",
    "    merged_df = merged_df.rename(columns={\"Population (2016 est.)[8]\": \"Population\"})\n",
    "    \n",
    "    # Convert columns to appropriate data types\n",
    "    merged_df[['W', 'L', 'Population']] = merged_df[['W', 'L', 'Population']].astype(float)\n",
    "\n",
    "    # Group by metropolitan area and calculate average win/loss ratio\n",
    "    win_loss_df = merged_df.groupby(\"Metropolitan area\").agg({'W': 'mean', 'L': 'mean', 'Population': 'mean'})\n",
    "    win_loss_df[\"WinLossRatio\"] = win_loss_df[\"W\"] / (win_loss_df[\"W\"] + win_loss_df[\"L\"])\n",
    "\n",
    "    return win_loss_df\n",
    "\n",
    "\n",
    "def sports_team_performance():\n",
    "    \n",
    "    NHL_2018 = clean_and_merge_df('NHL', nhl_df, cities)\n",
    "    NBA_2018 = clean_and_merge_df('NBA', nba_df, cities)\n",
    "    MLB_2018 = clean_and_merge_df('MLB', mlb_df, cities)\n",
    "    NFL_2018 = clean_and_merge_df('NFL', nfl_df, cities)\n",
    "    \n",
    "    # Note: p_values is a full dataframe, so df.loc[\"NFL\",\"NBA\"] should be the same as df.loc[\"NBA\",\"NFL\"] and\n",
    "    # df.loc[\"NFL\",\"NFL\"] should return np.nan\n",
    "    sports = ['NHL', 'NBA', 'MLB', 'NFL']\n",
    "    p_values = pd.DataFrame({k:np.nan for k in sports}, index=sports)\n",
    "    \n",
    "    for sport1 in sports:\n",
    "        for sport2 in sports:\n",
    "            if sport1 != sport2:\n",
    "                common_cities = set(eval(f'{sport1}_2018.index')) & set(eval(f'{sport2}_2018.index'))\n",
    "                array1 = eval(f'{sport1}_2018.loc[{sport1}_2018.index.isin(common_cities), \"WinLossRatio\"]')\n",
    "                array2 = eval(f'{sport2}_2018.loc[{sport2}_2018.index.isin(common_cities), \"WinLossRatio\"]')\n",
    "                _, p_value = stats.ttest_rel(array1, array2)\n",
    "                p_values.loc[sport1, sport2] = p_value\n",
    "    \n",
    "    assert abs(p_values.loc[\"NBA\", \"NHL\"] - 0.02) <= 1e-2, \"The NBA-NHL p-value should be around 0.02\"\n",
    "    #assert abs(p_values.loc[\"MLB\", \"NFL\"] - 0.80) <= 1e-2, \"The MLB-NFL p-value should be around 0.80\"\n",
    "    return p_values\n",
    "\n",
    "# Call the function\n",
    "p_values_result = sports_team_performance()\n",
    "print(p_values_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35880310-2a69-4c30-9119-ac96697b6345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
